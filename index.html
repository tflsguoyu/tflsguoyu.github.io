<!DOCTYPE html>
<html>
	<head>
		<meta http-equiv="content-type" content="text/html; charset=UTF-8"/>
		<link rel="stylesheet" type="text/css" href="css/style.css" media="screen"/>
		<title>GuoYu's Homepage</title>
	</head>

	<body>
		<div class="center_wrapper">
		
			<br>			
<!-- 			<div id="navigation">
				<ul>
					<li class="current_page_item"><a href="index.html"><b>Home</b></a></li>
					<li><a href="webpage/publication.html">Publications</a></li>
					<li><a href="webpage/pdf/GUOYU_CV.pdf" target="_blank">CV (pdf)</a></li>
					<li><a href="webpage/selfstudy.html"><font color="white">Blog</font></a></li>
				</ul>
			</div> -->
						
			<!-- <hr>			 -->
			<br>
			
			<img src="webpage/image/me.png" width="200" height="200" alt="" class="left bordered"/>

			<br>

			<h4>Guo, Yu (<a href="webpage/image/gallery/me.html" style="text-decoration: none; color:inherit;" target="_blank">郭煜</a>) 
				<span style="padding-left:300px">
					<a href="webpage/pdf/GUOYU_CV.pdf" target="_blank">CV (.pdf)</a>
			</h4>
			<br>
			<h6>Senior Researcher</h6>
			<br>
			<br>
			<h5>Tencent Game AI Research Center</h5>
			<h5>Los Angeles, US</h5>
			<br>
			<br>

			<p>tflsguoyu(at)gmail(dot)com</p>
															
			<br>
			<center>
				<a href="http://en.csu.edu.cn" target="_blank"><img src = "webpage/image/CSU.png" border=0 height=25 alt="Central South University (Changsha, China)"></a> B.A
				&rarr;
				<a href="http://english.cas.cn/" target="_blank"><img src="webpage/image/UCAS.png" border=0 height=25 alt="University of Chinese Academy of Science (Beijing, China)"></a> M.A 
				&rarr;
				<a href="webpage/image/gallery/siat.html" target="_blank"><img src ="webpage/image/SIAT.png" border=0 height=25 alt="Shenzhen Institute of Advanced Technology (Shenzhen, China)"></a> M.Phil
				&rarr;
				<a href="webpage/image/gallery/ntu.html" target="_blank"><img src="webpage/image/NTU.png" border=0 height=25 alt="Nanyang Technological University (Singapore)"></a> R.A
				&rarr;
				<a href="webpage/image/gallery/uci.html" target="_blank"><img src="webpage/image/UCI.png" border=0 height=25 alt="University of California, Irvine (CA, US)"></a> Ph.D
				(
				<a href="webpage/image/gallery/autodesk.html" target="_blank"><img src="webpage/image/Autodesk.png" border=0 height=25 alt="Autodesk (CA, US)"></a> Intern
				&rarr;
				<a href="webpage/image/gallery/megvii.html" target="_blank"><img src="webpage/image/megvii.png" border=0 height=25 alt="Megvii (CA, US)"></a> Intern
				&rarr;
				<a href="webpage/image/gallery/adobe.html" target="_blank"><img src="webpage/image/adobe.jpg" border=0 height=25 alt="Adobe (CA, US)"></a> Intern
				&rarr;
				<a href="webpage/image/gallery/facebook.html" target="_blank"><img src="webpage/image/facebook.png" border=0 height=25 alt="Facebook (CA, US)"></a> Intern)
				&rarr;
				<a href="webpage/image/gallery/tencent.html" target="_blank"><img src="webpage/image/tencent.png" border=0 height=25 alt="Tencent (CA, US)"></a> Researcher
				
			</center>
				
			<br>

			<p>
			I am a senior researcher at Tencent Games AI Research Center based in Los Angeles, US. I received my Ph.D in Computer Science from <a href="http://www.uci.edu/" target="_blank">University of California, Irvine</a> in 2021, supervised by <a href="https://shuangz.com/" target="_blank">ZHAO Shuang</a>. 
			My interests include Computer Graphics and Vision, especially in material appearance modeling and physically based rendering. 
			</p>

			<p>
			Before starting Ph.D, I was a member of <a href="webpage/image/btc.jpg" target="_blank">BeingThere Centre</a> and worked on several research projects at Nanyang Technological University (Singapore) as a Research Associate. 
			</p>
			
			<p>
			I obtained my M.S in CS from Shenzhen Institute of Advanced Technology, Chinese Academy Sciences (Shenzhen, China), advised by <a href="http://www.cse.cuhk.edu.hk/~pheng/" target="_blank">HENG Pheng-Ann</a>.		

			And B.S in Mathematics from Central South University (Changsha, China).
			</p>

			<p>
			
			</p>


<!-- seperate line -->
			<h5>Publications:</h5>

<!-- seperate line -->
			<img src="webpage/image/2021TOG.jpg" width="200" height="100" alt="" class="left bordered"/>
			<p>
				<b>Beyond Mie Theory: Systematic Computation of Bulk Scattering Parameters based on Microphysical Wave Optics</b>  
				<br>
				Yu Guo, Adrian Jarabo, Shuang Zhao
				<br>
				<i>ACM Transactions on Graphics (SIGGRAPH Asia 2021), 2021</i>
				<br>
				[<a href="javascript:;" onclick="myFun('abs_2021TOG');">abstract</a>]
				[<a href="webpage/pdf/2021TOG.pdf" target="_blank">paper</a>]
				[<a href="https://github.com/tflsguoyu/waveoptics" target="_blank">code</a>]
			</p>	
			<div id="abs_2021TOG" class="abs2" style="display:none;">
				Light scattering in participating media and translucent materials is typically modeled using the radiative transfer theory. Under the assumption of independent scattering between particles, it utilizes several bulk scattering parameters to statistically characterize light-matter interactions at the macroscale. To calculate these parameters based on microscale material properties, the Lorenz-Mie theory has been considered the gold standard. In this paper, we present a generalized framework capable of systematically and rigorously computing bulk scattering parameters beyond the far-field assumption of Lorenz-Mie theory. Our technique accounts for microscale wave-optics effects such as diffraction and interference as well as interactions between nearby particles. Our framework is general, can be plugged in any renderer supporting Lorenz-Mie scattering, and allows arbitrary packing rates and particles correlation; we demonstrate this generality by computing bulk scattering parameters for a wide range of materials, including anisotropic and correlated media.
			</div>
			<div class="anchor"></div>

<!-- seperate line -->
			<img src="webpage/image/2020TOG.jpg" width="200" height="100" alt="" class="left bordered"/>
			<p>
				<b>MaterialGAN: Reflectance Capture using a Generative SVBRDF Model</b>  
				<br><br>
				Yu Guo, Cameron Smith, Miloš Hašan, Kalyan Sunkavalli, Shuang Zhao
				<br>
				<i>ACM Transactions on Graphics (SIGGRAPH Asia 2020), 2020</i>
				<br>
				[<a href="javascript:;" onclick="myFun('abs_2020TOG');">abstract</a>]
				[<a href="webpage/pdf/2020TOG.pdf" target="_blank">paper</a>]
				[<a href="webpage/suppl/2020TOG/index.html" target="_blank">suppl</a>]
				[<a href="https://youtu.be/fD6CTb1DlbE" target="_blank">fastforward</a>]
				[<a href="https://youtu.be/CrAoVsJf0Zw" target="_blank">presentation</a>]
				[<a href="webpage/pdf/2020TOG_main.pptx" target="_blank">slides</a>]
				[<a href="https://github.com/tflsguoyu/materialgan" target="_blank">code</a>]
			</p>	
			<div id="abs_2020TOG" class="abs2" style="display:none;">
				We address the problem of reconstructing spatially-varying BRDFs from a small set of image measurements. This is a fundamentally under-constrained problem, and previous work has relied on using various regularization priors or on capturing many images to produce plausible results. In this work, we present MaterialGAN, a deep generative convolutional network based on StyleGAN2, trained to synthesize realistic SVBRDF parameter maps. We show that MaterialGAN can be used as a powerful material prior in an inverse rendering framework: we optimize in its latent representation to generate material maps that match the appearance of the captured images when rendered. We demonstrate this framework on the task of reconstructing SVBRDFs from images captured under flash illumination using a hand-held mobile phone. Our method succeeds in producing plausible material maps that accurately reproduce the target images, and outperforms previous state-of-the-art material capture methods in evaluations on both synthetic and real data. Furthermore, our GAN-based latent space allows for high-level semantic material editing operations such as generating material variations and material morphing.
			</div>
			<div class="anchor"></div>

<!-- seperate line -->
			<img src="webpage/image/2020CGF.jpg" width="200" height="100" alt="" class="left bordered"/>
			<p>
				<b>A Bayesian Inference Framework for Procedural Material Parameter Estimation</b>  
				<br><br>
				Yu Guo, Miloš Hašan, Lingqi Yan, Shuang Zhao
				<br>
				<i>Computer Graphics Forum (Pacific Graphics 2020), 2020</i>
				<br>
				[<a href="javascript:;" onclick="myFun('abs_2020bayesian');">abstract</a>]
				[<a href="webpage/pdf/2020CGF_poster.pdf" target="_blank">poster</a>]
				[<a href="webpage/pdf/2020CGF.pdf" target="_blank">paper</a>]
				[<a href="webpage/suppl/2020CGF/index.html" target="_blank">suppl</a>]
				[<a href="https://youtu.be/fQPBqHKJmWQ" target="_blank">fastforward</a>]
				[<a href="webpage/pdf/2020CGF_ff.pptx" target="_blank">slides</a>]
			</p>	
			<div id="abs_2020bayesian" class="abs2" style="display:none;">
				Procedural material models have been gaining traction in many applications thanks to their flexibility, compactness, and easy editability. In this paper, we explore the inverse rendering problem of procedural material parameter estimation from photographs using a Bayesian framework. We use summary functions for comparing unregistered images of a material under known lighting, and we explore both hand-designed and neural summary functions. In addition to estimating the parameters by optimization, we introduce a Bayesian inference approach using Hamiltonian Monte Carlo to sample the space of plausible material parameters, providing additional insight into the structure of the solution space. To demonstrate the effectiveness of our techniques, we fit procedural models of a range of materials---wall plaster, leather, wood, anisotropic brushed metals and metallic paints---to both synthetic and real target images.
			</div>
			<div class="anchor"></div>


<!-- seperate line -->
			<img src="webpage/image/2018TOG.png" width="200" height="100" alt="" class="left bordered"/>
			<p>
				<b>Position-Free Monte Carlo Simulation for Arbitrary Layered BSDFs</b>  
				<br><br>
				Yu Guo, Miloš Hašan, Shuang Zhao
				<br>
				<i>ACM Transactions on Graphics (SIGGRAPH Asia 2018), 2018</i>
				<br>
				[<a href="javascript:;" onclick="myFun('abs_2018TOG');">abstract</a>]
				[<a href="https://github.com/tflsguoyu/layeredbsdf_paper/blob/master/layeredbsdf.pdf" target="_blank">paper</a>]
				[<a href="https://tflsguoyu.github.io/layeredbsdf_suppl/" target="_blank">suppl</a>]
				[<a href="https://github.com/tflsguoyu/layeredbsdf" target="_blank">code</a>]
				[<a href="webpage/pdf/2018TOG_poster.pdf" target="_blank">poster</a>]
				[<a href="https://youtu.be/v5u6LYCN_PU" target="_blank">fastforward</a>]
				[<a href="webpage/pdf/2018TOG_main.pptx" target="_blank">slides</a>]
				[<a href="https://youtu.be/Bv3yat484aQ" target="_blank">Two Minute Papers</a>]
			</p>	
			<div id="abs_2018TOG" class="abs2" style="display:none;">
				Real-world materials are often layered: metallic paints, biological tissues, and many more. Variation in the interface and volumetric scattering properties of the layers leads to a rich diversity of material appearances from anisotropic highlights to complex textures and relief patterns. However, simulating light-layer interactions is a challenging problem. Past analytical or numerical solutions either introduce several approximations and limitations, or rely on expensive operations on discretized BSDFs, preventing the ability to freely vary the layer properties spatially. We introduce a new unbiased layered BSDF model based on Monte Carlo simulation, whose only assumption is the layer assumption itself. Our novel position-free path formulation is fundamentally more powerful at constructing light transport paths than generic light transport algorithms applied to the special case of flat layers, since it is based on a product of solid angle instead of area measures, so does not contain the high-variance geometry terms needed in the standard formulation. We introduce two techniques for sampling the position-free path integral, a forward path tracer with next-event estimation and a full bidirectional estimator. We show a number of examples, featuring multiple layers with surface and volumetric scattering, surface and phase function anisotropy, and spatial variation in all parameters.
			</div>
			<div class="anchor"></div>

<!-- seperate line -->
			<hr style="width:100%;text-align:left;margin-left:0">

<!-- seperate line -->
			<img src="webpage/image/2017CGA.png" width="200" height="100" alt="" class="left bordered"/>	
			<p>
				<b>A Virtual Try-on System for Prescription Eyeglasses</b>  
				<br><br>
				Qian Zhang, Yu Guo, Pierre-Yves Laffont, Tobias Martin, Markus Gross
				<br>
				<i>IEEE Computer Graphics and Application, 2017</i>
				<br>
				[<a href="javascript:;" onclick="myFun('abs_2017CGA');">abstract</a>]
				[<a href="webpage/pdf/2017CGA.pdf" target="_blank">paper</a>]
				[<a href="https://youtu.be/_fckwZCzCgc" target="_blank">video</a>]
			</p>		
			<div id="abs_2017CGA" class="abs2" style="display:none;">
				Corrective lenses introduce distortion caused by the refraction effect, which changes the wearer’s appearance. To give users a more realistic experience, a virtual try-on system modifies an input video and virtually inserts prescription eyeglasses with reflections and shading, producing an output similar to a virtual mirror.
			</div>		
			<div class="anchor"></div>

<!-- seperate line -->
			<img src="webpage/image/2017VisionResearch.png" width="200" height="100" alt="" class="left bordered"/>			
			<p>
				<b>3D Faces are Recognized More Accurately and Faster than 2D Faces, but with Similar Inversion Effects</b>  
				<br><br>
				Derric Eng, Belle Yick, Yu Guo, Hong Xu, Miriam Reiner, TJ Cham, SH Chen
				<br>
				<i>Vision Research, 2017</i>
				&nbsp;
				[<a href="javascript:;" onclick="myFun('abs_2017VisionResearch');">abstract</a>]
				[<a href="webpage/pdf/2017VisionResearch.pdf" target="_blank">paper</a>]
				[<a href="webpage/pdf/2015APCV_poster.pdf" target="_blank">poster</a>]
			</p>
			<div id="abs_2017VisionResearch" class="abs2" style="display:none;">
				Recognition of faces typically occurs via holistic processing where individual features are combined to provide an overall facial representation. However, when faces are inverted, there is greater reliance on featural processing where faces are recognized based on their individual features. These findings are based on a substantial number of studies using 2-dimensional (2D) faces and it is unknown whether these results can be extended to 3-dimensional (3D) faces, which have more depth information that is absent in the typical 2D stimuli used in face recognition literature. The current study used the face inversion paradigm as a means to investigate how holistic and featural processing are differentially influenced by 2D and 3D faces. Twenty-five participants completed a delayed face-matching task consisting of upright and inverted faces that were presented as both 2D and 3D stereoscopic images. Recognition accuracy was significantly higher for 3D upright faces compared to 2D upright faces, providing support that the enriched visual information in 3D stereoscopic images facilitates holistic processing that is essential for the recognition of upright faces. Typical face inversion effects were also obtained, regardless of whether the faces were presented in 2D or 3D. Moreover, recognition performances for 2D inverted and 3D inverted faces did not differ. Taken together, these results demonstrated that 3D stereoscopic effects influence face recognition during holistic processing but not during featural processing. Our findings therefore provide a novel perspective that furthers our understanding of face recognition mechanisms, shedding light on how the integration of stereoscopic information in 3D faces influences face recognition processes.
			</div>
			<div class="anchor"></div>

<!-- seperate line -->
			<img src="webpage/image/2016CGF.png" width="200" height="100" alt="" class="left bordered"/>		
			<p>
				<b>Physically Based Video Editing</b>  
				<br><br>
				Jean-Charles Bazin, Claudia Pluss, Yu Guo, Tobias Martin, Alec Jacobson, Markus Gross
				<br>
				<i>Computer Graphics Forum (Pacific Graphics 2016), 2016</i>
				<br>
				[<a href="javascript:;" onclick="myFun('abs_2016CGF');">abstract</a>]
				[<a href="webpage/pdf/2016CGF.pdf" target="_blank">paper</a>]
				[<a href="https://youtu.be/bBzmlCU5FEo" target="_blank">video</a>]
			</p>
			<div id="abs_2016CGF" class="abs2" style="display:none;">
				Convincing manipulation of objects in live action videos is a difficult and often tedious task. Skilled video editors achieve this with the help of modern professional tools, but complex motions might still lack physical realism since existing tools do not consider the laws of physics. On the other hand, physically based simulation promises a high degree of realism, but typically creates a virtual 3D scene animation rather than returning an edited version of an input live action video. We propose a framework that combines video editing and physics-based simulation. Our tool assists unskilled users in editing an input image or video while respecting the laws of physics and also leveraging the image content. We first fit a physically based simulation that approximates the object’s motion in the input video. We then allow the user to edit the physical parameters of the object, generating a new physical behavior for it. The core of our work is the formulation of an image-aware constraint within physics simulations. This constraint manifests as external control forces to guide the object in a way that encourages proper texturing at every frame, yet producing physically plausible motions. We demonstrate the generality of our method on a variety of physical interactions: rigid motion, multi-body collisions, clothes and elastic bodies.
			</div>
			<div class="anchor"></div>			
			
<!-- seperate line -->			
			<img src="webpage/image/2013MICCAI.png" width="200" height="100" alt="" class="left bordered"/>			
			<p>
				<b>GPU Accelerated CBCT Reconstruction From Few Views with SART and TV Regularization</b>  
				<br><br>
				Ping Liu, Lin Shi, Defeng Wang, Yu Guo, Jianying Li, Jing Qin, Pheng-Ann Heng
				<br>
				<i>HPC-MICCAI, 2013</i>
				&nbsp;
				[<a href="javascript:;" onclick="myFun('abs_2013MICCAI');">abstract</a>]
				[<a href="webpage/pdf/2013MICCAI.pdf" target="_blank">paper</a>]
			</p>
			<div id="abs_2013MICCAI" class="abs2" style="display:none;">
				Compressed sensing-based iterative algorithms can reconstruct high-quality CBCT from undersampled and noisy projection data. However, a practical implementation of these methods still remains a challenge due to the heavy computation. We implemented an algorithm by combining simultaneous algebraic reconstruction technique (SART) and total variation (TV) regularization for the CBCT reconstruction from few views. More importantly, we introduced approaches to fit the SART and TV into the GPU architecture. Experimental results showed that our GPU accelerated algorithm could obtain good reconstruction quality from 20 to 40 projections, as well as significant gain in time performance. It only took 29.1s for reconstruction from 120 projections with 40 iterations. The proposed method has potential to make iterative-based CBCT reconstruction more accessable for routine clinical applications.
			</div>
			<div class="anchor"></div>
				
<!-- seperate line -->			
			<img src="webpage/image/2013ICIP.png" width="200" height="100" alt="" class="left bordered"/>	
			<p>
				<b>Real-time Hand Detection Based on Multi-stage HOG-SVM Classifier</b>  
				<br><br>
				Jiang Guo, Jun Cheng, Jianxin Pang, Yu Guo
				<br>
				<i>International Conference on Image Processing (ICIP), 2013</i>
				<br>
				[<a href="javascript:;" onclick="myFun('abs_2013ICIP');">abstract</a>]
				[<a href="webpage/pdf/2013ICIP.pdf" target="_blank">paper</a>]
			</p>
			<div id="abs_2013ICIP" class="abs2" style="display:none;">
				In this paper, we propose a real-time hand detection method with multi-stage HOG-SVM classifier. Unlike traditional methods based on learning which make decomposition of feature vector or combination of different types of features or classifiers, upon the division of background into several categories, we propose a multi-stage classifier which combines several SVM classifies each of which is trained to distinguish corresponding divisions of background and target. Furthermore, in order to improve speed performance, skin color in-formation and integral histogram are also applied. Experiment results demonstrate that the proposed algorithm works well under multiple challenging backgrounds in real-time speed (16 frames per second).
			</div>
			<div class="anchor"></div>

<!-- seperate line -->						
			<img src="webpage/image/2013ICIA.png" width="200" height="100" alt="" class="left bordered"/>			
			<p>
				<b>A GPU-accelerated Finite Element Solver for Simulation of Soft-body Deformation</b>  
				<br><br>
				Yu Guo, Jianying Li, Ping Liu, Qiong Wang, Jing Qin
				<br>
				<i>International Conference on Information and Automation (ICIA), 2013</i>
				<br>
				[<a href="javascript:;" onclick="myFun('abs_2013ICIA');">abstract</a>]
				[<a href="webpage/pdf/2013ICIA.pdf" target="_blank">paper</a>]
			</p>
			<div id="abs_2013ICIA" class="abs2" style="display:none;">
				A nonlinear physical simulation is presented involving the soft body deformation and interaction contacts. We demonstrate the finite element method relying on Lagrangian discretization to simulate the deformation of the soft body with hyperelastic material properties. To perform a stable simulation, we use the constrained Delaunay Tetrahedralization to resampling and remeshing the object. A new contact strategy is developed and used to replace the collision detection. This method does not need to iteratively achieve the optimal contact response on the constrained boundary. It can dynamically determine whether the contact status of the point should be in a static or a sliding friction mode. The explicit method for the finite element model is employed in order to perform all the steps of the algorithm on the GPUs and achieve a real-time simulation.
			</div>	
			<div class="anchor"></div>

<!-- seperate line -->									
			<img src="webpage/image/2013JIT.png" width="200" height="100" alt="" class="left bordered"/>			
			<p>
				<b>A Survey on Simulation of Soft Tissue Deformation in Virtual Surgery (in Chinese)</b>  
				<br><br>
				Yu Guo, Jing Qin
				<br>
				<i>Journal of Integration Technology, 2013</i>
				<br>
				[<a href="javascript:;" onclick="myFun('abs_2013JIT');">abstract</a>]
				[<a href="webpage/pdf/2013JIT.pdf" target="_blank">paper</a>]
			</p>
			<div id="abs_2013JIT" class="abs2" style="display:none;">
				Human soft tissues generally exhibit complex material properties such as nonlinearity, anisotropy, incompressibility and viscoelastictity. Soft tissue deformation is one of the most important yet difficult research tasks in virtual surgery. This paper presents a comprehensive survey on simulation of soft tissue deformation in virtual surgery. We first give an introduction of the virtual surgery system. Then we detailed variours methods from geometrically-based methods to physically-based methods, from mesh-based models to meshless models. Finally, we describe some promissing research directions on this topic.
			</div>	
			<div class="anchor"></div>

<!-- seperate line -->													
			<img src="webpage/image/2012SigAsia.png" width="200" height="100" alt="" class="left bordered"/>			
			<p>
				<b>Fall Over or Sliding Down?</b>  
				<br><br>
				Yu Guo.
				<br>
				<i>Siggraph Asia (Poster), 2012</i>
				<br>
				[<a href="javascript:;" onclick="myFun('abs_2012SigAsia');">abstract</a>]
				[<a href="webpage/pdf/2012SigAsia.pdf" target="_blank">paper</a>]
				[<a href="webpage/pdf/2012SigAsia_poster.pdf" target="_blank">poster</a>]
			</p>
			<div id="abs_2012SigAsia" class="abs2" style="display:none;">
				We present a contact model depending on constraints coupled with frictions which is called portable frictional contact model. Although it does not reach the optimal result, the flexibility and convenience of our method make it an easier use in contact simulation.
			</div>
			<div class="anchor"></div>
			
<!-- seperate line -->																	
			<img src="webpage/image/2012IROS.png" width="200" height="100" alt="" class="left bordered"/>		
			<p>
				<b>A Master-Slave Robotic Simulator Based on GPUDirect</b>  
				<br><br>
				Jianying Li, Yu Guo, Heye Zhang, Yongming Xie
				<br>
				<i>International Conference on Intelligent Robots and Systems (IROS), 2012</i>
				<br>
				[<a href="javascript:;" onclick="myFun('abs_2012IROS');">abstract</a>]
				[<a href="webpage/pdf/2012IROS.pdf" target="_blank">Paper</a>]
			</p>
			<div id="abs_2012IROS" class="abs2" style="display:none;">
				The same as in traditional surgery, surgeons in telerobotic surgery need extensive training to achieve experience and highly accurate instrument manipulation. Traditional training methods like practice in operating room have major drawbacks such as high risk and limited opportunity for which virtual reality (VR) and computer technologies can offer solutions. To accelerate the data transmission speed in our master-slave robotic simulator, GPUDirect was applied to ensure the synchronization and display rate of three computers. By using GPUDirect with InfiniBand card we realized up to 247% performance improvement in data transmission speed on NVIDIA Tesla™ products on different computers compared to that without GPUDirect, which shows that GPUDirect enables better communication between remote GPUs over InfiniBand.
			</div>
			<div class="anchor"></div>

<!-- seperate line -->																	
			<h5>Thesis:</h5>
		
<!-- seperate line -->																		
			<img src="webpage/image/UCI2.png" width="200" height="100" alt="" class="left bordered"/>			
			<p>
				<b>Multi-scale Appearance Modeling of Complex Materials</b>  
				<br><br>
				Ph.D.
				<br>
				<i>August, 2021</i>
				<br>
				[<a href="webpage/pdf/Thesis_PhD.pdf" target="_blank">Paper</a>]
				[<a href="webpage/pdf/Thesis_PhD (slides).pdf" target="_blank">Slides</a>]	
			</p>
			<div class="anchor"></div>

<!-- seperate line -->																		
			<img src="webpage/image/UCAS-SIAT.png" width="200" height="100" alt="" class="left bordered"/>			
			<p>
				<b>GPU-based Soft Body Deformation with Nonlinear Finite Element Method</b>  
				<br><br>
				Master
				<br>
				<i>June, 2013</i>
				<br>
				[<a href="webpage/pdf/Thesis_Master.pdf" target="_blank">Paper (In Chinese)</a>]
				[<a href="webpage/pdf/Thesis_Master (slides).pdf" target="_blank">Slides (In Chinese)</a>]	
			</p>
			<div class="anchor"></div>
			
<!-- seperate line -->																						
			<img src="webpage/image/CSU-Math.png" width="200" height="100" alt="" class="left bordered"/>		
			<p>
				<b>Forces Distribution with Fractal Theory in High Velocity Compaction Technology</b>  
				<br><br>
				Bachelor
				<br>
				<i>June, 2010</i>
				<br>
				[<a href="webpage/pdf/Thesis_Bachelor.pdf" target="_blank">Paper (In Chinese)</a>]
				[<a href="webpage/pdf/Thesis_Bachelor (slides).pdf" target="_blank">Slides (In Chinese)</a>]
			</p>
			<div class="anchor"></div>
		

			<a href="https://info.flagcounter.com/Abqv"><img src="https://s01.flagcounter.com/count2/Abqv/bg_FFFFFF/txt_000000/border_FFFFFF/columns_5/maxflags_20/viewers_0/labels_0/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0"></a>
				
		</div>	

		<script>
			function myFun(fn_paper) {
				var x = document.getElementById(fn_paper);
				if (x.style.display == "none") {
					x.style.display = "block";
				} else {
					x.style.display = "none";
				}
			}
		</script>

	</body>
</html>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-91620117-1', 'auto');
  ga('send', 'pageview');

</script>